
<html>
    <link rel="stylesheet" type="text/css" href="style-note.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title> Gradient descent </title>

    <body>

      	<div class='note'>
          <h1> The basics </h1>
          <p>
          A Gaussian process is a stochastic process (collection of random variables)
          such that every finite collection of those random variables has a multivariate
          normal distribution. The distribution generated by a Gaussian process is
          the joint distribution of all those (infinitely many) random variables.
          </p>
          <div class='mark'>
            The Gaussian process may be fully specified as \(N ( E(x), k(x,x) )\),
            where E(x) is the expected value as a function of x and k is the
            covariance matrix (or kernel) at x.
          </div>
          <p>
	        A typical kernel selection can be:
	        \[ k(x_i, x_j) = \sigma_f^2 e^{-\frac{1}{2 l^2}||x_i,x_j||_2^2} \]
	        The parameter \(\sigma_f\) affects vertical variation of the function,
          while \(l\) affects the smoothness of the function. In higher dimensions
          (forecast dependent on more than one variable) it would be possible to
          select different weights \(l\) for each feature. Note that there is a
          requirement for the kernel to be symmetric and positive-definite.<br><br>

	        The mean is typically set to zero, as an appropriatelly selected kernel
          can capture any relevant characteristics as well.<br><br>

	        Assuming some points relating to the function are known (the training set)
          and predictions are required at a given point then:
	        \[ p(y_f, y_0) = \mathcal{N}
	        (\begin{bmatrix} m_f \\ m_0 \end{bmatrix},
	         \begin{bmatrix} k(x_f,x_f) & k(x_f,x_0) \\ k(x_0,x_f)^T & k(x_0,x_0) \end{bmatrix} ) \]
	        \[ p(y_f | y_0) = \mathcal{N} ( m_f + k(x_f,x_0) \cdot k(x_0,x_0)^{-1} \cdot (y_0-m_0) , k(x_f,x_f) - k(x_f,x_0) \cdot k(x_0,x_0)^{-1} \cdot k(x_f,x_0)^T ) \]

	        The parameters should be selected so that they maximize the likelihood
          that \(x_0,y_0\) are derived from the given gaussian process. This is
          equivalent to maximizing the log likelihood, which is defined as:
	        \[log p(f|x) = -0.5 y_0^T (K + \sigma_n^2 I) y_0 - 0.5 log|K+\sigma_n^2 I| - 0.5nlog2\pi \]
	        Where \(n\) is the number of points.<br><br>

	        For a limited amount of data the challenge is to identify the most
          relevant kernel and fit its parameters to the data. For larger data
          sets, the inversion of the data matrix required for the prediction
          can make things computationally intractable.<br>
	        One possible solution is using a smaller pseudo-data set produced by
          the training data-set, which would still provide sufficient information
          for the gaussian process to fit the data sufficiently well.<br><br>
          </p>
	    </div>

      <div class='note'>
	        <div style='float:right'> <img src='images/ml_gaussianCombinedKernels.png'></div>

	        <h2>Selecting a kernel </h2>
          <p>
			    To approximate data outside the range covered by available samples a
          prerequisite is to make certain assumptions regarding the form of the
          function. The gaussian process with the standard quadratic exponential
          kernel will typically revert to a zero mean normally distributed prediction
           when trying to predict a value for from previous datapoints. This can be avoided
           by combining different kernels as shown in the figure. <br>
          </p>

	    </div>


	    <div class='note'>
	        <div style='float:right'> <img src='images/ml_gaussianVariationalInference.png'></div>

	        <h2>Stochastic variational inference with Gaussian processes </h2>
          <p>
			    The idea behind the variational inference approach is that instead of
          using the initial dataset \((x_0,y_0)\), one may approximate it by a
          smaller set of points \((u,z)\) which may be optimised along with the
          kernel parameters. The main difficulty is that the log likelihood
          function set as an optimization objective above may no longer be used.
          Instead a quantity known as evidence lower bound is used.<br>
          </p>

	    </div>



	    <script>

          x1 = [-2,-1.8,-1.6,-1.4,-1.2,-1,-0.8,-0.6,-0.4,-0.2,0,
                0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2]
          y1 = [4,3.24,2.56,1.96,1.44,1,0.64,0.36,0.16,0.04,0,
                0.04,0.16,0.36,0.64,1,1.44,1.96,2.56,3.24,4]

          x2 = [-1.50,1.20,-0.96,0.77,-0.61,0.49,-0.39,0.31,-0.25,0.20]
          y2 = [2.250,1.440,0.922,0.590,0.377,0.242,0.155,0.099,0.063,0.041]

          FIG1 = document.getElementById('img-gradient-descent');
    			Plotly.newPlot(FIG1,
                         [{x: x1, y: y1,
                           name: 'function'},
                          {x: x2, y: y2,
                           name: 'solution trajectory'}],
                          {margin: { t:50, b:50, l:50, r:10},
                           title: 'Gradient descent on simple quadratic function',
                           xaxis: {'title' : 'x', 'range': [-2,2]},
                           yaxis: {'title' : 'f(x)', 'range': [-0.001, 2.4]},
                           font: {'size':10}},
                        );

		  </script>

    </body>
</html>
