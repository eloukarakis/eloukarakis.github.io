<!DOCTYPE html>
<html>
    <link rel="stylesheet" type="text/css" href="style-note.css">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title> Gradient descent </title>

    <body>

      	<div class='note'>
            <div class='img'>
                <iframe class='img' src="figures/algo_gaussianProcess_base.html"></iframe>
            </div>

            <h1> The basics </h1>
            <p>
            A Gaussian process is a stochastic process (collection of random variables)
            such that every finite collection of those random variables has a multivariate
            normal distribution. The distribution generated by a Gaussian process is
            the joint distribution of all those (infinitely many) random variables.
            </p>
            <div class='mark'>
                The Gaussian process may be fully specified as \(N ( E(x), k(x,x) )\),
                where E(x) is the expected value as a function of x and k is the
                covariance matrix (or kernel) at x.
            </div>
            <p>
    	    A typical kernel selection can be:
    	    \[ k(x_i, x_j) = \sigma_f^2 e^{-\frac{1}{2 l^2}||x_i,x_j||_2^2} \]
    	    The parameter \(\sigma_f\) affects vertical variation of the function,
            while \(l\) affects the smoothness of the function. In higher dimensions
            (forecast dependent on more than one variable) it would be possible to
            select different weights \(l\) for each feature. Note that there is a
            requirement for the kernel to be symmetric and positive-definite.<br><br>

    	    The mean is typically set to zero, as an appropriatelly selected kernel
            can capture any relevant characteristics as well.<br><br>

    	    Assuming some points relating to the function are known (the training set)
            and predictions are required at a given point then:
    	    \[ p(y_f, y_0) = \mathcal{N}
    	    (\begin{bmatrix} m_f \\ m_0 \end{bmatrix},
    	    \begin{bmatrix} k(x_f,x_f) & k(x_f,x_0) \\ k(x_0,x_f)^T & k(x_0,x_0) \end{bmatrix} ) \]
    	    \[ p(y_f | y_0) = \mathcal{N} ( m_f + k(x_f,x_0) \cdot k(x_0,x_0)^{-1}
            \cdot (y_0-m_0) , k(x_f,x_f) - k(x_f,x_0) \cdot k(x_0,x_0)^{-1} \cdot k(x_f,x_0)^T ) \]

    	    The parameters should be selected so that they maximize the likelihood
            that \(x_0,y_0\) are derived from the given gaussian process. This is
            equivalent to maximizing the log likelihood, which is defined as:
    	    \[log p(f|x) = -0.5 y_0^T (K + \sigma_n^2 I) y_0 - 0.5 log|K+\sigma_n^2 I| - 0.5nlog2\pi \]
    	    Where \(n\) is the number of points.<br><br>

    	    For a limited amount of data the challenge is to identify the most
            relevant kernel and fit its parameters to the data. For larger data
            sets, the inversion of the data matrix required for the prediction
            can make things computationally intractable.<br>
    	    One possible solution is using a smaller pseudo-data set produced by
            the training data-set, which would still provide sufficient information
            for the gaussian process to fit the data sufficiently well.<br>
            Taking this idea a step further, the stochastic variational inference
            approach approximates the original dataset via a smaller set of points
            \((u,z)\) which is optimised along with the kernel parameters.<br><br>

            Finally, to approximate data outside range covered by available samples a
            prerequisite is to make certain assumptions regarding the form of the
            function. The gaussian process with the standard quadratic exponential
            kernel will typically revert to a zero mean normally distributed prediction
            when trying to predict a value for from previous datapoints. This can be avoided
            by combining different kernels.
            </p>
	    </div>

    </body>
</html>
