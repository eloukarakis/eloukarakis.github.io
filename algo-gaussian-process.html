
<html>
    <link rel="stylesheet" type="text/css" href="style-note.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title> Gradient descent </title>

    <body>

      	<div class='note'>
          <h1> The basics </h1>

          A Gaussian process is a stochastic process (collection of random variables)
          such that every finite collection of those random variables has a multivariate
          normal distribution. The distribution generated by a Gaussian process is
          the joint distribution of all those (infinitely many) random variables.

          <div class='mark'>
            The Gaussian process may be fully specified as \(N ( E(x), k(x,x) )\),
            where E(x) is the expected value as a function of x and k is the
            covariance matrix (or kernel) at x.
          </div>

	        A typical kernel selection can be:
	        \[ k(x_i, x_j) = \sigma_f^2 e^{-\frac{1}{2 l^2}||x_i,x_j||_2^2} \]
	        The parameter \(\sigma_f\) affects vertical variation of the function,
          while \(l\) affects the smoothness of the function. In higher dimensions
          (forecast dependent on more than one variable) it would be possible to
          select different weights \(l\) for each feature. Note that there is a
          requirement for the kernel to be symmetric and positive-definite.<br><br>

	        The mean is typically set to zero, as an appropriatelly selected kernel
          can capture any relevant characteristics as well.<br><br>

	        Assuming some points relating to the function are known (the training set)
          and predictions are required at a given point then:
	        \[ p(y_f, y_0) = \mathcal{N}
	        (\begin{bmatrix} m_f \\ m_0 \end{bmatrix},
	         \begin{bmatrix} k(x_f,x_f) & k(x_f,x_0) \\ k(x_0,x_f)^T & k(x_0,x_0) \end{bmatrix} ) \]
	        \[ p(y_f | y_0) = \mathcal{N} ( m_f + k(x_f,x_0) \cdot k(x_0,x_0)^{-1} \cdot (y_0-m_0) , k(x_f,x_f) - k(x_f,x_0) \cdot k(x_0,x_0)^{-1} \cdot k(x_f,x_0)^T ) \]

	        The parameters should be selected so that they maximize the likelihood
          that \(x_0,y_0\) are derived from the given gaussian process. This is
          equivalent to maximizing the log likelihood, which is defined as:
	        \[log p(f|x) = -0.5 y_0^T (K + \sigma_n^2 I) y_0 - 0.5 log|K+\sigma_n^2 I| - 0.5nlog2\pi \]
	        Where \(n\) is the number of points.<br><br>

	        For a limited amount of data the challenge is to identify the most
          relevant kernel and fit its parameters to the data. For larger data
          sets, the inversion of the data matrix required for the prediction
          can make things computationally intractable.<br>
	        One possible solution is using a smaller pseudo-data set produced by
          the training data-set, which would still provide sufficient information
          for the gaussian process to fit the data sufficiently well.<br><br>

	    </div>

      <div class='note'>
	        <div style='float:right'> <img src='images/ml_gaussianCombinedKernels.png'></div>

	        <h2>Selecting a kernel </h2>

			To approximate data outside the range covered by available samples a prerequisite is to make certain assumptions regarding the form of the function. The gaussian process with the standard quadratic exponential kernel will typically revert to a zero mean normally distributed prediction when trying to predict a value for from previous datapoints. This can be avoided by combining different kernels as shown in the figure. <br>



	    </div>


	    <div class='note'>
	        <div style='float:right'> <img src='images/ml_gaussianVariationalInference.png'></div>

	        <h2>Stochastic variational inference with Gaussian processes </h2>

			The idea behind the variational inference approach is that instead of using the initial dataset \((x_0,y_0)\), one may approximate it by a smaller set of points \((u,z)\) which may be optimised along with the kernel parameters. The main difficulty is that the log likelihood function set as an optimizatio objective above may no longer be used. Instead a quantity known as evidence lower bound is used.<br>

      <div class='note'>
          <h2> Useful references </h2>
          <ul class='lit'>
            <li> C. E. Rasmussen, C. K. I. Williams (2006). <u>Gaussian Processes for Machine Learning</u>. MIT Press. </li>
            <li> E. Snelson, Z. Ghahramani (2005). <u>Sparse Gaussian Processes using Pseudo-Inputs</u>. Neural Information Processing Systems Conference.</li>
            <li> J. Hensman, N. Fusi, N. D. Lawrence (2013). <u>Gaussian Processes for Big Data</u>. Conference on Uncertainty in Artificial Intelligence.</li>
          </ul>
      </div>

	    </div>



	    <script>

          x1 = [-2,-1.8,-1.6,-1.4,-1.2,-1,-0.8,-0.6,-0.4,-0.2,0,
                0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2]
          y1 = [4,3.24,2.56,1.96,1.44,1,0.64,0.36,0.16,0.04,0,
                0.04,0.16,0.36,0.64,1,1.44,1.96,2.56,3.24,4]

          x2 = [-1.50,1.20,-0.96,0.77,-0.61,0.49,-0.39,0.31,-0.25,0.20]
          y2 = [2.250,1.440,0.922,0.590,0.377,0.242,0.155,0.099,0.063,0.041]

          FIG1 = document.getElementById('img-gradient-descent');
    			Plotly.newPlot(FIG1,
                         [{x: x1, y: y1,
                           name: 'function'},
                          {x: x2, y: y2,
                           name: 'solution trajectory'}],
                          {margin: { t:50, b:50, l:50, r:10},
                           title: 'Gradient descent on simple quadratic function',
                           xaxis: {'title' : 'x', 'range': [-2,2]},
                           yaxis: {'title' : 'f(x)', 'range': [-0.001, 2.4]},
                           font: {'size':10}},
                        );

		  </script>

    </body>
</html>
