<!DOCTYPE html>
<html>
    <link rel="stylesheet" type="text/css" href="style-note.css">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title> Ensemble learning </title>

    <body>

      	<div class='note'>
          <h1> Ensembles </h1>
          <p> Ensemble learning is about using multiple different models to
            generate a prediction.
          </p>

	    </div>

        <div class='note'>

            <h2> Voting classifiers / averaging regressors </h2>
            <p> Assuming there a few trained models in case of classification
                one may use as a prediction the result that most of the models
                give, or for regression the average of all models. If some
                performance metrics are known about the different models, then
                the weighting of different models may be accordingly adjusted.
            </p>
        </div>

        <div class='note'>
            <h2> Bagging </h2>
                One way to obtain different models it to simply use the same method
                but on different subsets of the training data. The subsets may be
                generated by randomly sampling the overall training set. If the
                sampling is done with replacement then it is called bagging
                (bootstrap aggregating), if it is done without replacement it is
                called pasting. <br>
                Each predictor may be expected to have a higher
                bias than if it were trained on the overall set. However, the
                aggregation of the results will reduce both this bias. At the same
                time the ensemble results can be expected to have lower variance
                compared to that of a single predictor trained on the entire set.
                When generating the models it is also be possible to randomly
                sample training features in addition to training data.<br>
                Bagging will likely lead to a subset of samples not actually
                used at all in training. This may be used for an out-of-bag
                evaluation of the prediction performance, thus negating the
                need for cross-validation.<br>
        </div>

        <div class='note'>
            <h2> Boosting </h2>
            <p> This involves incrementaly building a more accurate model by
                building a new model instance that places more weight on the
                errors of the previous models. One common approach is AdaBoost
                which at each step weights the training data set to based on the
                prediction error so far. Another approach is gradiend boosting
                which works by sequentially adding predictors to an ensemble,
                each correcting its predecessor. However, instead of tweaking
                the instance weights like AdaBoost, this approach tries to fit
                the new predictor to the residual errors of the previous predictor.
            </p>
        </div>

        <div class='note'>
            <h2> Stacking </h2>
            <p> This involves training a model for aggregating the predictions
                of all models in the ensemble.
            </p>
        </div>

    </body>
</html>
