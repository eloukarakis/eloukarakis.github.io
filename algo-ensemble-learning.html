<!DOCTYPE html>
<html>
    <link rel="stylesheet" type="text/css" href="style-note.css">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title> Ensemble learning </title>

    <body>

      	<div class='note'>
          <h1> Ensembles </h1>
          <p> Ensemble learning is about using the output of multiple different
              models to generate a prediction. Even if the models used do not
              perform particularly well, an ensemble predictions can be expected
              to be better.
          </p>

	    </div>

        <div class='note'>

            <h2> Voting classifiers / averaging regressors </h2>
            <p> Assuming there a few trained models in case of classification
                one may use as a prediction the result that most of the models
                give, or for regression the average of all models. If some
                performance metrics are known about the different models, then
                the weighting of different models may be accordingly adjusted.
            </p>
        </div>

        <div class='note'>
            <h2> Bagging </h2>
            <p> One way to obtain different models it to simply use the same method
                but on different subsets of the training data. The subsets may be
                generated by randomly sampling the overall training set. If the
                sampling is done with replacement then it is called bagging
                (bootstrap aggregating), if it is done without replacement it is
                called pasting. <br>
                Each predictor may be expected to have a higher
                bias than if it were trained on the overall set. However, the
                aggregation of the results will reduce both this bias. At the same
                time the ensemble results can be expected to have lower variance
                compared to that of a single predictor trained on the entire set.
                When generating the models it is also be possible to randomly
                sample training features in addition to training data.<br>
                Bagging will likely lead to a subset of samples not actually
                used at all in training. This may be used for an out-of-bag
                evaluation of the prediction performance, thus negating the
                need for cross-validation.<br><br>

                Random forests are one example of bagging application, where
                multiple decision trees are trained for subsets of the training
                data-set and features.<br>
            </p>
        </div>

        <div class='note'>
            <h2> Boosting </h2>
            <p> This involves incrementaly building a more accurate model by
                building a new model instance that places more weight on the
                errors of the previous models. One common approach is AdaBoost
                which at each step weighs the training data set based on the
                prediction error so far. Another approach is gradiend boosting
                which works by sequentially adding predictors to an ensemble,
                each correcting its predecessor. However, instead of tweaking
                the instance weights like AdaBoost, this approach tries to fit
                the new predictor to the residual errors of the previous predictor.<br><br>

                Compared to the random forest mentioned above, a gradient
                boosting tree regresson will sequentialy fit decistion trees on
                the forecasting error so far.<br>
            </p>
        </div>

        <div class='note'>
            <h2> Stacking </h2>
            <p> This involves training a model for aggregating the predictions
                of all models in the ensemble.
            </p>
        </div>

    </body>
</html>
